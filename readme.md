# LLM Proxy with Keywords Management GUI

### Architecture / Architektura

#### Flow Diagram / Schemat przepywu

```mermaid
flowchart LR
    %% Style definitions
    classDef userStyle fill:#2563eb,stroke:#1d4ed8,color:white,rx:10,ry:10
    classDef systemStyle fill:#4f46e5,stroke:#4338ca,color:white,rx:8,ry:8
    classDef checkStyle fill:#7c3aed,stroke:#6d28d9,color:white,rx:15,ry:15
    classDef processStyle fill:#8b5cf6,stroke:#7c3aed,color:white,rx:5,ry:5
    classDef serviceStyle fill:#2dd4bf,stroke:#14b8a6,color:white,rx:10,ry:10
    classDef responseStyle fill:#10b981,stroke:#059669,color:white,rx:10,ry:10

    %% Nodes
    U([ User]):::userStyle --> R[ Request]:::systemStyle
    R --> K{Keyword Check}:::checkStyle
    
    subgraph proxy [" LLM Proxy System "]
        direction TB
        K --> |Found| C[Ч Clean Prompt]:::processStyle
        K --> |Not Found| F[ Full Context]:::processStyle
        
        C --> FW[( Flowise Service)]:::serviceStyle
        F --> LL[( LLama.cpp Service)]:::serviceStyle
    end
    
    FW --> Resp[ Response Handler]:::responseStyle
    LL --> Resp
    Resp --> U

    %% Subgraph style
    style proxy fill:transparent,stroke:#94a3b8,stroke-width:2px

    %% Add notes
    subgraph notes [" "]
        direction TB
        n1[Clean Prompt removes emotional tags and history context]:::noteStyle
        n2[Full Context keeps conversation history and all tags]:::noteStyle
    end
    style notes fill:none,stroke:none
    classDef noteStyle fill:#fef3c7,stroke:#fcd34d,color:black,rx:5,ry:5
    
    %% Connect notes
    C -.- n1
    F -.- n2
```
**[EN]**  
The diagram shows the main flow of the application:
1. User sends a request
2. System checks for keywords
3. If keywords are found, the request is cleaned and sent to Flowise
4. If no keywords are found, the full context is sent to LLama.cpp
5. Response is returned to the user

**[PL]**  
Diagram pokazuje g贸wny przepyw w aplikacji:
1. U偶ytkownik wysya zapytanie
2. System sprawdza sowa kluczowe
3. Jeli znaleziono sowa kluczowe, zapytanie jest czyszczone i wysyane do Flowise
4. Jeli nie znaleziono s贸w kluczowych, peny kontekst jest wysyany do LLama.cpp
5. Odpowied藕 jest zwracana do u偶ytkownika

[English](#english) | [Polski](#polski)

---
<a name="english"></a>
## English

### Table of Contents
- [About](#about)
- [Key Features](#key-features)
- [Requirements](#requirements)
- [Installation](#installation)
- [Usage](#usage)
- [Architecture](#architecture)
- [Troubleshooting](#troubleshooting)
- [License Information](#license-information)
- [Contributing](#contributing)

### About
LLM Proxy is a tool that enables intelligent request routing between llama.cpp and Flowise based on keywords. It features a simple web interface for managing routing rules and provides a seamless integration layer between different LLM services.

### Key Features
- Intelligent request routing between llama.cpp and Flowise
- Web-based keyword management interface
- Conversation context preservation for llama.cpp
- Context isolation for Flowise requests
- Automatic removal of emotion tags [xxx] for Flowise
- Persistent configuration storage in SQLite
- Full Docker containerization
- Support for streaming responses
- Multi-language support

### Requirements
- Docker and Docker Compose
- Running llama.cpp server
- Running Flowise server
- Python 3.9+
- 2GB RAM minimum
- 10GB disk space

### Installation
1. Clone the repository
```bash
git clone https://github.com/PPTG/llm-proxy-llamacpp-flowise.git
cd llm-proxy-llamacpp-flowise
```

2. Configure environment variables in docker-compose.yml
```yaml
environment:
  - LLAMA_URL=http://your-llama-server:8988
  - FLOWISE_URL=http://your-flowise-server:3000
```

3. Create environment file
```bash
cp .env.example .env
# Edit .env with your settings
```

4. Run the application
```bash
docker compose up --build
```

5. Verify installation
```bash
curl http://localhost:8444/health
```

### Usage

#### Available Endpoints
- GUI: `http://localhost:5555` - keyword management interface
- API: `http://localhost:8444` - proxy endpoints for llama.cpp and Flowise

#### API Documentation
All endpoints with examples:

##### Adding a keyword
```bash
curl -X POST http://localhost:8444/api/keywords \
  -H "Content-Type: application/json" \
  -d '{
    "keyword": "temperature",
    "flowise_id": "your-flow-id",
    "description": "Temperature query"
  }'
```

##### Sending a query
```bash
curl -X POST http://localhost:8444/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {
        "role": "user",
        "content": "What is the temperature?"
      }
    ]
  }'
```

#### GUI Management
1. Open `http://localhost:5555` in your browser
2. Navigate to the keyword management section
3. Add, edit, or delete keywords
4. Each keyword requires:
   - Detection phrase
   - Flowise flow ID
   - Optional description
   - Optional active/inactive status
### Architecture

#### Project Structure
```
/
 app.py            # Main FastAPI application file
 gui.py           # Flask application for web interface
 requirements.txt  # Python dependencies
 Dockerfile       # Docker image configuration
 docker-compose.yml # Services configuration
 tests/           # Test suite
 docs/           # Additional documentation
 templates/       # HTML templates for GUI
     index.html   # Main interface template
```

#### How it Works
1. Request Processing:
   - System receives incoming request
   - Extracts the latest query from conversation context
   - Removes emotion tags if present
   - Checks for keyword matches

2. Routing Logic:
   - If keyword match found:
     * Forwards clean query to Flowise
     * Returns streaming response
   - If no match found:
     * Forwards full context to llama.cpp
     * Maintains conversation history

3. Response Handling:
   - Handles streaming responses
   - Manages error states
   - Ensures consistent format

### Troubleshooting

#### Common Issues
1. Connection Errors
   - Check if services are running
   - Verify URLs in configuration
   - Ensure network connectivity
   - Check firewall settings

2. Database Issues
   - Check /data directory permissions
   - Verify Docker volume mounting
   - Ensure SQLite is working properly

3. Performance Issues
   - Monitor memory usage
   - Check system resources
   - Verify connection speeds

#### Debug Mode
To enable debug mode:
```bash
docker compose -f docker-compose.debug.yml up
```

#### Logs
Access logs via:
```bash
docker compose logs -f
```

<a name="polski"></a>
## Polski

### Spis treci
- [O projekcie](#o-projekcie)
- [G贸wne funkcje](#g贸wne-funkcje)
- [Wymagania](#wymagania)
- [Instalacja](#instalacja)
- [U偶ytkowanie](#u偶ytkowanie)
- [Architektura](#architektura)
- [Rozwizywanie problem贸w](#rozwizywanie-problem贸w)
- [Informacje o licencji](#informacje-o-licencji)
- [Wsp贸praca](#wsp贸praca)

### O projekcie
LLM Proxy to narzdzie umo偶liwiajce inteligentne przekierowywanie zapyta midzy llama.cpp a Flowise na podstawie s贸w kluczowych. Wyposa偶one jest w prosty interfejs webowy do zarzdzania reguami przekierowa oraz zapewnia pynn integracj midzy r贸偶nymi usugami LLM.

### G贸wne funkcje
- Inteligentne przekierowywanie zapyta midzy llama.cpp a Flowise
- Interfejs webowy do zarzdzania sowami kluczowymi
- Zachowanie kontekstu rozmowy dla llama.cpp
- Izolacja kontekstu dla zapyta do Flowise
- Automatyczne usuwanie znacznik贸w emocji [xxx] dla Flowise
- Persystentne przechowywanie konfiguracji w SQLite
- Pena konteneryzacja w Docker
- Obsuga odpowiedzi strumieniowych
- Wsparcie dla wielu jzyk贸w

### Wymagania
- Docker i Docker Compose
- Dziaajcy serwer llama.cpp
- Dziaajcy serwer Flowise
- Python 3.9+
- Minimum 2GB RAM
- 10GB przestrzeni dyskowej

### Instalacja
1. Sklonuj repozytorium
```bash
git clone https://github.com/PPTG/llm-proxy-llamacpp-flowise.git
cd llm-proxy-llamacpp-flowise
```

2. Skonfiguruj zmienne rodowiskowe w docker-compose.yml
```yaml
environment:
  - LLAMA_URL=http://your-llama-server:8988
  - FLOWISE_URL=http://your-flowise-server:3000
```

3. Utw贸rz plik rodowiskowy
```bash
cp .env.example .env
# Edytuj .env zgodnie z twoimi ustawieniami
```

4. Uruchom aplikacj
```bash
docker compose up --build
```

5. Zweryfikuj instalacj
```bash
curl http://localhost:8444/health
```

### U偶ytkowanie

#### Dostpne endpointy
- GUI: `http://localhost:5555` - interfejs zarzdzania sowami kluczowymi
- API: `http://localhost:8444` - endpointy proxy dla llama.cpp i Flowise

#### Dokumentacja API
Wszystkie endpointy z przykadami:

##### Dodawanie sowa kluczowego
```bash
curl -X POST http://localhost:8444/api/keywords \
  -H "Content-Type: application/json" \
  -d '{
    "keyword": "temperatura",
    "flowise_id": "your-flow-id",
    "description": "Zapytanie o temperatur"
  }'
```

##### Wysyanie zapytania
```bash
curl -X POST http://localhost:8444/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {
        "role": "user",
        "content": "Jaka jest temperatura?"
      }
    ]
  }'
```

### Architektura

#### Struktura projektu
```
/
 app.py            # G贸wny plik aplikacji FastAPI
 gui.py           # Aplikacja Flask do interfejsu webowego
 requirements.txt  # Zale偶noci Pythona
 Dockerfile       # Konfiguracja obrazu Docker
 docker-compose.yml # Konfiguracja usug
 tests/           # Testy
 docs/           # Dodatkowa dokumentacja
 templates/       # Szablony HTML dla GUI
     index.html   # G贸wny szablon interfejsu
```

#### Jak to dziaa
1. Przetwarzanie zapyta:
   - System otrzymuje przychodzce zapytanie
   - Wyodrbnia najnowsze zapytanie z kontekstu rozmowy
   - Usuwa znaczniki emocji, jeli wystpuj
   - Sprawdza dopasowania s贸w kluczowych

2. Logika routingu:
   - Jeli znaleziono dopasowanie sowa kluczowego:
     * Przekazuje oczyszczone zapytanie do Flowise
     * Zwraca odpowied藕 strumieniow
   - Jeli nie znaleziono dopasowania:
     * Przekazuje peny kontekst do llama.cpp
     * Zachowuje histori rozmowy

### Rozwizywanie problem贸w

#### Czste problemy
1. Bdy poczenia
   - Sprawd藕 czy usugi s uruchomione
   - Zweryfikuj adresy URL w konfiguracji
   - Upewnij si, 偶e jest poczenie sieciowe
   - Sprawd藕 ustawienia firewalla

2. Problemy z baz danych
   - Sprawd藕 uprawnienia katalogu /data
   - Zweryfikuj montowanie wolumenu Docker
   - Upewnij si, 偶e SQLite dziaa prawidowo

3. Problemy z wydajnoci
   - Monitoruj u偶ycie pamici
   - Sprawd藕 zasoby systemowe
   - Zweryfikuj prdkoci pocze

#### Tryb debugowania
Aby wczy tryb debugowania:
```bash
docker compose -f docker-compose.debug.yml up
```

#### Logi
Dostp do log贸w:
```bash
docker compose logs -f
```

---
<a name="license-information"></a>
## License Information / Informacje o licencjach

### Project License / Licencja projektu
Apache License 2.0

### Third Party Licenses / Licencje komponent贸w zewntrznych

This project uses several third-party libraries and components, each with its own license:

| Component | License | Usage |
|-----------|---------|-------|
| llama.cpp | MIT License | Large Language Model backend |
| Flowise | Apache License 2.0 | Flow-based automation |
| FastAPI | MIT License | Main API framework |
| Flask | BSD License | Web GUI framework |
| httpx | BSD License | HTTP client |
| SQLite | Public Domain | Database |
| Pydantic | MIT License | Data validation |
| uvicorn | BSD License | ASGI server |

Full license text can be found in the LICENSE file in the repository.

## Contributing / Wsp贸praca
We welcome contributions! Please see our Contributing Guidelines for details.
Zapraszamy do wsp贸pracy! Szczeg贸y znajduj si w Wytycznych dla Wsp贸tw贸rc贸w.

---

Created with わ by PPTG







